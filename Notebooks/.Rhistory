x[3,] = c(BIC(m1),BIC(m2),BIC(m3))
x[4,] = c(rmse(m1),rmse(m2),rmse(m3))
colnames(rlts)  = c("Modelo 1","Modelo 2","Modelo 3")
row.names(rlts) = c("logLik","AIC","BIC","RMSE")
ft = flextable(data.frame(x))
autofit(ft)
#| label: tbl-btp
#| tbl-cap: "Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 5,000 iteraciones"
x = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975))
x = data.frame( t(x) )
x$pars = c("intercepto","hp","wt","vs","gear","carb")
colnames(x) = c("q2.5%","Median","q97.5%","parámetros")
ft = flextable(x[c(4,1,2,3)])
autofit(ft)
x = matrix(0,nrow = 4,ncol = 3)
x[1,] = c(logLik(m1),logLik(m2),logLik(m3))
x[2,] = c(AIC(m1),AIC(m2),AIC(m3))
x[3,] = c(BIC(m1),BIC(m2),BIC(m3))
x[4,] = c(rmse(m1),rmse(m2),rmse(m3))
x = data.frame(x)
x$pars =  c("logLik","AIC","BIC","RMSE")
colnames(rlts)  = c("Criterio","Modelo 1","Modelo 2","Modelo 3")
x = matrix(0,nrow = 4,ncol = 3)
x[1,] = c(logLik(m1),logLik(m2),logLik(m3))
x[2,] = c(AIC(m1),AIC(m2),AIC(m3))
x[3,] = c(BIC(m1),BIC(m2),BIC(m3))
x[4,] = c(rmse(m1),rmse(m2),rmse(m3))
x = data.frame(x)
View(x)
x$pars =  c("logLik","AIC","BIC","RMSE")
View(x)
colnames(x)  = c("Modelo 1","Modelo 2","Modelo 3","Criterio")
ft = flextable(x[c(4,1,2,3)])
x = matrix(0,nrow = 4,ncol = 3)
x[1,] = c(logLik(m1),logLik(m2),logLik(m3))
x[2,] = c(AIC(m1),AIC(m2),AIC(m3))
x[3,] = c(BIC(m1),BIC(m2),BIC(m3))
x[4,] = c(rmse(m1),rmse(m2),rmse(m3))
x = data.frame(x)
x$pars =  c("logLik","AIC","BIC","RMSE")
colnames(x)  = c("Modelo 1","Modelo 2","Modelo 3","Criterio")
ft = flextable(x[c(4,1,2,3)])
autofit(ft)
rmse(m1)
rmse(m2)
sum(m1$residuals)
sum(m1$residuals)/length(m1$residuals)
r = sum(m1$residuals)/length(m1$residuals)
sqrt(r)
#| code-fold: true
rmse = function(m){
r = m$residuals
mse = sum(m$residuals^2)/length(m$residuals)
return(sqrt(mse))
}
#| label: tbl-criteria
#| tbl-cap: "Criterios de informaciooo4ón de los modelos. Se selecciona el modelo con menores criterios."
x = matrix(0,nrow = 4,ncol = 3)
x[1,] = c(logLik(m1),logLik(m2),logLik(m3))
x[2,] = c(AIC(m1),AIC(m2),AIC(m3))
x[3,] = c(BIC(m1),BIC(m2),BIC(m3))
x[4,] = c(rmse(m1),rmse(m2),rmse(m3))
x = data.frame(x)
x$pars =  c("logLik","AIC","BIC","RMSE")
colnames(x)  = c("Modelo 1","Modelo 2","Modelo 3","Criterio")
ft = flextable(x[c(4,1,2,3)])
autofit(ft)
r = sum(m1$residuals^2)/length(m1$residuals)
e
r
sqrt(r)
r = sum(m2$residuals^2)/length(m2$residuals)
r
sqrt(r)
#| code-fold: true
rmse = function(m){
mse = sum(m$residuals^2)/length(m$residuals)
return(sqrt(mse))
}
#| label: tbl-criteria
#| tbl-cap: "Criterios de informaciooo4ón de los modelos. Se selecciona el modelo con menores criterios."
x = matrix(0,nrow = 4,ncol = 3)
x[1,] = c(logLik(m1),logLik(m2),logLik(m3))
x[2,] = c(AIC(m1),AIC(m2),AIC(m3))
x[3,] = c(BIC(m1),BIC(m2),BIC(m3))
x[4,] = c(rmse(m1),rmse(m2),rmse(m3))
x = data.frame(x)
x$pars =  c("logLik","AIC","BIC","RMSE")
colnames(x)  = c("Modelo 1","Modelo 2","Modelo 3","Criterio")
ft = flextable(x[c(4,1,2,3)])
autofit(ft)
rm(list = ls())
x = seq(1,21,5)
x
x = seq(1,21,3)
x
x[1]
x[2]
x = seq(1,n,k)
install.packages("caret")
library(caret)
#| message: false
library(flextable)
library(GGally)
library(bayesplot)
library(ggplot2)
library(ggfortify)
df = mtcars[,c(1,4,6,8,10,11)]
str(df)
kfld = createFolds(y = df[,1],k = k)
kfld = createFolds(y = df[,1],k = 5)
kfold = createFolds(y = df[,1],k = 5)
kfold
kfold = createFolds(y = df[,1],k = 5,returnTrain = TRUE)
kfold
kfold = createFolds(y = df[,1],k = 5,list = FALSE,returnTrain = TRUE)
kfold
kfold = createFolds(y = df[,1],k = 5,list = FALSE)
kfold
kfold = createFolds(y = df[,1],k = 5)
kfold
kfold = createFolds(y = df[,1],k = 1)
kfold
kfold
kfold = createFolds(y = df[,1],k = 5)
kfold
m1 = lm(mpg~vs+hp+gear+carb,data = df[-kfold$Fold1,])
m2 = lm(mpg~hp+gear+carb, data = df[-kfold$Fold1,])
m3 = lm(mpg~wt+gear+carb, data = df[-kfold$Fold1,])
m1
summary(m1)
df[-kfold$Fold1,]
dim(df[-kfold$Fold1,])
dim(df[-kfold$Fold2,])
dim(df[-kfold$Fold5,])
dim(df[-kfold$Fold4,])
dim(df[-kfold$Fold3,])
predict(object = m1,newdata = df[kfold$Fold1,])
p1 = predict(object = m1,newdata = df[kfold$Fold1,])
RMSE(p1,obs = df[kfold$Fold1,1])
MAE(p1,obs = df[kfold$Fold1,1])
p1 = predict(object = m1,newdata = df[kfold$Fold1,],interval = 0.9)
?predict()
?predict.lm()
p1 = predict(object = m1,newdata = df[kfold$Fold1,],interval ="prediction")
p1
p1 = predict(object = m1,newdata = df[kfold$Fold1,])
p1
kfold = function(df,k){
# Generar la particion
kfld = createFolds(df[,1],k = k)
mat = NULL
for (i in 1:k) {
# separar los datos en conjuntos de prueba y entrenamiento
dfE= df[-kfld[[i]],]
dfP = df[kfld[[i]],]
# Ajustar los modelos
m1 = lm(mpg~vs+hp+gear+carb,data = dfE)
m2 = lm(mpg~hp+gear+carb,data = dfE)
m3 = lm(mpg~wt+gear+carb,data = dfE)
p1 = predict(m1,dfP)
p2 = predict(m2,dfP)
p2 = predict(m3,dfP)
# Calcular AIC y RMSE
aic = c(
AIC(m1),
AIC(m2),
AIC(m3)
)
rmse = c(
RMSE(pred =  p1,obs = dfP[,1]),
RMSE(pred =  p2,obs = dfP[,1]),
RMSE(pred =  p3,obs = dfP[,1])
)
mae = c(
MAE(pred =  p1,obs = dfP[,1]),
MAE(pred =  p2,obs = dfP[,1]),
MAE(pred =  p3,obs = dfP[,1])
)
mape = c(
mean(abs((p1-dfP[,1])/dfP[,1])),
mean(abs((p2-dfP[,1])/dfP[,1])),
mean(abs((p3-dfP[,1])/dfP[,1]))
)
# Unir los datos
mat = rbind(mat,c(aic,rmse,mae,mape))
}
colnames(mat) = c("AIC1","AIC2","AIC3","RMSE1","RMSE2","RMSE3","MAE1","MAE2",
"MAE3","MAPE1","MAPE2","MAPE3")
row.names(mat) = NULL
return(mat)
}
kfold = function(df,k){
# Generar la particion
kfld = createFolds(df[,1],k = k)
mat = NULL
for (i in 1:k) {
# separar los datos en conjuntos de prueba y entrenamiento
dfE= df[-kfld[[i]],]
dfP = df[kfld[[i]],]
# Ajustar los modelos
m1 = lm(mpg~vs+hp+gear+carb,data = dfE)
m2 = lm(mpg~hp+gear+carb,data = dfE)
m3 = lm(mpg~wt+gear+carb,data = dfE)
p1 = predict(m1,dfP)
p2 = predict(m2,dfP)
p2 = predict(m3,dfP)
# Calcular AIC y RMSE
aic = c(
AIC(m1),
AIC(m2),
AIC(m3)
)
rmse = c(
RMSE(pred =  p1,obs = dfP[,1]),
RMSE(pred =  p2,obs = dfP[,1]),
RMSE(pred =  p3,obs = dfP[,1])
)
mae = c(
MAE(pred =  p1,obs = dfP[,1]),
MAE(pred =  p2,obs = dfP[,1]),
MAE(pred =  p3,obs = dfP[,1])
)
mape = c(
mean(abs((p1-dfP[,1])/dfP[,1])),
mean(abs((p2-dfP[,1])/dfP[,1])),
mean(abs((p3-dfP[,1])/dfP[,1]))
)
# Unir los datos
mat = rbind(mat,c(aic,rmse,mae,mape))
}
colnames(mat) = c("AIC1","AIC2","AIC3","RMSE1","RMSE2","RMSE3","MAE1","MAE2",
"MAE3","MAPE1","MAPE2","MAPE3")
row.names(mat) = NULL
return(mat)
}
#| message: false
library(GGally)
library(ggplot2)
library(flextable)
library(bayesplot)
library(ggfortify)
#| message: false
library(GGally)
library(ggplot2)
library(flextable)
library(bayesplot)
library(ggfortify)
#| code-fold: true
kfold = function(df,k){
# Generar la particion
kfld = createFolds(df[,1],k = k)
mat = NULL
for (i in 1:k) {
# separar los datos en conjuntos de prueba y entrenamiento
dfE= df[-kfld[[i]],]
dfP = df[kfld[[i]],]
# Ajustar los modelos
m1 = lm(mpg~vs+hp+gear+carb,data = dfE)
m2 = lm(mpg~hp+gear+carb,data = dfE)
m3 = lm(mpg~wt+gear+carb,data = dfE)
p1 = predict(m1,dfP)
p2 = predict(m2,dfP)
p2 = predict(m3,dfP)
# Calcular AIC y RMSE
aic = c(
AIC(m1),
AIC(m2),
AIC(m3)
)
rmse = c(
RMSE(pred =  p1,obs = dfP[,1]),
RMSE(pred =  p2,obs = dfP[,1]),
RMSE(pred =  p3,obs = dfP[,1])
)
mae = c(
MAE(pred =  p1,obs = dfP[,1]),
MAE(pred =  p2,obs = dfP[,1]),
MAE(pred =  p3,obs = dfP[,1])
)
mape = c(
mean(abs((p1-dfP[,1])/dfP[,1])),
mean(abs((p2-dfP[,1])/dfP[,1])),
mean(abs((p3-dfP[,1])/dfP[,1]))
)
# Unir los datos
mat = rbind(mat,c(aic,rmse,mae,mape))
}
colnames(mat) = c("AIC1","AIC2","AIC3","RMSE1","RMSE2","RMSE3","MAE1","MAE2",
"MAE3","MAPE1","MAPE2","MAPE3")
row.names(mat) = NULL
return(mat)
}
rm(list = ls())
#| message: false
library(GGally)
library(ggplot2)
library(flextable)
library(bayesplot)
library(ggfortify)
#| code-fold: true
kfold = function(df,k){
# Generar la particion
kfld = createFolds(df[,1],k = k)
mat = NULL
for (i in 1:k) {
# separar los datos en conjuntos de prueba y entrenamiento
dfE= df[-kfld[[i]],]
dfP = df[kfld[[i]],]
# Ajustar los modelos
m1 = lm(mpg~vs+hp+gear+carb,data = dfE)
m2 = lm(mpg~hp+gear+carb,data = dfE)
m3 = lm(mpg~wt+gear+carb,data = dfE)
p1 = predict(m1,dfP)
p2 = predict(m2,dfP)
p2 = predict(m3,dfP)
# Calcular AIC y RMSE
aic = c(
AIC(m1),
AIC(m2),
AIC(m3)
)
rmse = c(
RMSE(pred =  p1,obs = dfP[,1]),
RMSE(pred =  p2,obs = dfP[,1]),
RMSE(pred =  p3,obs = dfP[,1])
)
mae = c(
MAE(pred =  p1,obs = dfP[,1]),
MAE(pred =  p2,obs = dfP[,1]),
MAE(pred =  p3,obs = dfP[,1])
)
mape = c(
mean(abs((p1-dfP[,1])/dfP[,1])),
mean(abs((p2-dfP[,1])/dfP[,1])),
mean(abs((p3-dfP[,1])/dfP[,1]))
)
# Unir los datos
mat = rbind(mat,c(aic,rmse,mae,mape))
}
colnames(mat) = c("AIC1","AIC2","AIC3","RMSE1","RMSE2","RMSE3","MAE1","MAE2",
"MAE3","MAPE1","MAPE2","MAPE3")
row.names(mat) = NULL
return(mat)
}
df = mtcars[,c(1,4,6,8,10,11)]
str(df)
rst = kfold(df = df,k = 5)
#| code-fold: true
kfold = function(df,k){
# Generar la particion
kfld = createFolds(df[,1],k = k)
mat = NULL
for (i in 1:k) {
# separar los datos en conjuntos de prueba y entrenamiento
dfE= df[-kfld[[i]],]
dfP = df[kfld[[i]],]
# Ajustar los modelos
m1 = lm(mpg~vs+hp+gear+carb,data = dfE)
m2 = lm(mpg~hp+gear+carb,data = dfE)
m3 = lm(mpg~wt+gear+carb,data = dfE)
p1 = predict(m1,dfP)
p2 = predict(m2,dfP)
p3 = predict(m3,dfP)
# Calcular AIC y RMSE
aic = c(
AIC(m1),
AIC(m2),
AIC(m3)
)
rmse = c(
RMSE(pred =  p1,obs = dfP[,1]),
RMSE(pred =  p2,obs = dfP[,1]),
RMSE(pred =  p3,obs = dfP[,1])
)
mae = c(
MAE(pred =  p1,obs = dfP[,1]),
MAE(pred =  p2,obs = dfP[,1]),
MAE(pred =  p3,obs = dfP[,1])
)
mape = c(
mean(abs((p1-dfP[,1])/dfP[,1])),
mean(abs((p2-dfP[,1])/dfP[,1])),
mean(abs((p3-dfP[,1])/dfP[,1]))
)
# Unir los datos
mat = rbind(mat,c(aic,rmse,mae,mape))
}
colnames(mat) = c("AIC1","AIC2","AIC3","RMSE1","RMSE2","RMSE3","MAE1","MAE2",
"MAE3","MAPE1","MAPE2","MAPE3")
row.names(mat) = NULL
return(mat)
}
rst = kfold(df = df,k = 5)
rst
t(rst)
summary*rst
summary(rst)
t(summary(rst))
summary(rst)
apply(rst,MARGIN = 2,FUN = "mean")
rst = kfold(df = df,k = 5)
apply(rst,MARGIN = 2,FUN = "quantile",probs = c(0.025,0.5,0.975))
t(apply(rst,MARGIN = 2,FUN = "quantile",probs = c(0.025,0.5,0.975)))
library(dplyr)
library(ggplot2)
library(cowplot)
library(flextable)
library(palmerpenguins)
#| label: tbl-penguins
#| tbl-cap: "Tabla de Contingencia para las especies de pingüinos en Palmer, Antártida."
x = table(penguins$species)
x = rbind(x,prop.table(x))
Total = c(344.0, 1.0)
Tipo = c("Frequencias","proporciones")
x = data.frame(cbind(round(x,2),Total))
x = cbind(Tipo,x)
ft = flextable(x)
autofit(ft)
#| label: tbl-pen-islands
#| tbl-cap: "Tabla de Contingencia para las especies de pingüinos por islas en Palmer, Antártida."
x = table(penguins$species,penguins$island)
x = addmargins(x)
Tipo = c("Adelie","Chinstrap","Gentoo","Total")
x = cbind(Tipo,x)
colnames(x) = c("Especie","Biscoe","Dream","Torgersen","Total")
ft = flextable(data.frame(x))
ft = add_header_row(ft, values = c(" ", "Isla"," "), colwidths = c(1,3,1))
autofit(ft)
#| label: tbl-pen-isla-sex
#| tbl-cap: "Tabla de Contingencia para las especies de pingüinos por islas y sexo en Palmer, Antártida."
library(dplyr)
x = ftable(penguins$sex,penguins$island,penguins$species)
x = addmargins(x)
sex = c(rep("Female",3),rep("Male",3),"Total")
isla = c(rep(c("Biscoe","Dream","Torgesen"),2),"Total")
x = cbind(cbind(sex,isla),data.frame(x))
names(x) = c("Sexo","Isla","Adelie","Chinstrap","Gentoo","Total")
row.names(x) = NULL
ft = as_grouped_data(x,groups = "Sexo")
flextable::as_flextable(ft,hide_grouplabel = TRUE) %>%
add_header_row(values = c(" ", "Especie"," "),colwidths = c(1,3,1))%>%
bold(j = 1, i = ~ !is.na(Sexo), bold = TRUE, part = "body") %>%
bold(part = "header", bold = TRUE) %>%
autofit()
#| label: tbl-contigen
#| tbl-cap: "Tabla de Contingencia para dos categorías X e Y."
x = data.frame(
X = c(paste0("X",c(1:3)),"...","Xk"),
y1 = c(paste0("n",c(1:3),",1"),"...","nk,1"),
y2 = c(paste0("n",c(1:3),",2"),"...","nk,2"),
y3 = c(paste0("n",c(1:3),",3"),"...","nk,3"),
yj = c(paste0("n",c(1:3),",j"),"...","nk,j"),
yc = c(paste0("n",c(1:3),",c"),"...","nk,c"),
yT = c(paste0("n",c(1:3),"."),"...","nk.")
)
Xt = c("Xt",paste0("n.",c(1:3)),"n.j","n.c","n")
x = rbind(x,Xt)
ft = flextable(x)
ft = add_header_row(ft, values = c(" ", "Y"," "), colwidths = c(1,5,1))
autofit(ft)
summary(rst)
rst = kfold(df = df,k = 5)
t(apply(rst,MARGIN = 2,FUN = "quantile",probs = c(0.025,0.5,0.975)))
rm(list = ls())
#| message: false
library(GGally)
library(ggplot2)
library(flextable)
library(bayesplot)
library(ggfortify)
df = mtcars[,c(1,4,6,8,10,11)]
str(df)
#| label: fig-pairs
#| fig-cap: "Gráfico de pares. La diagonal principal muestra histogramas densidades de cada una de las variables. La parte superior muestra el coeficiente de correlación entre dos variables, fila y columna. La parte inferior muestra un gráfico de dispersión entre dos variables."
ggpairs(df)
m1 = lm(mpg~.,data = df)
summary(m1)
m1 = lm(mpg~.,data = df)
summary(m1)
#| label: fig-btp
#| fig-cap: "Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B=5,000 iteraciones."
btp = lm_boots(y = df$mpg,x = as.matrix(df[,-1]),B = 5000)
lm_boots = function(y,x,B = 1000){
n = length(y)
est = NULL
for (i in 1:B) {
si = sample(x = 1:n,size = n,replace = TRUE)
mli = lm(y[si]~x[si,] )
ci = as.array(mli$coefficients)
est = rbind(est,ci)
}
# Estética
cn = colnames(x)
colnames(est) = c("intercepto",cn)
return(est)
}
#| label: fig-btp
#| fig-cap: "Gráfico de densidades. Cada densidad representa la distribución muestral aproximada para cada uno de los estimadores usando un Bootstrap de B=5,000 iteraciones."
btp = lm_boots(y = df$mpg,x = as.matrix(df[,-1]),B = 5000)
bayesplot_theme_set(theme_grey())
mcmc_dens(btp)+labs(title="Distribución muestral de los estimadores",
subtitle ="Bootstrap B = 5,000 iteraciones")
#| label: tbl-btp
#| tbl-cap: "Intervalos de confianza al 95%, obtenidos a partir de una muestra bootstrap de B = 5,000 iteraciones"
x = apply(btp,MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975))
# Estética
x = data.frame( t(x) )
x$pars = c("intercepto","hp","wt","vs","gear","carb")
colnames(x) = c("q2.5%","Median","q97.5%","parámetros")
ft = flextable(x[c(4,1,2,3)])
autofit(ft)
